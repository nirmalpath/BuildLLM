import numpy as np

# Create a 1-dimensional NumPy array with values from 0 to 9
arr = np.arange(10)
print("1D array:", arr)

# Create a 2-dimensional NumPy array with shape (2, 5)
arr_2d = np.arange(10).reshape((2, 5))
print("2D array:\n", arr_2d)

# Create two 2D arrays for matrix multiplication (shape: 2x3 and 3x2)
A = np.array([[1, 2, 3],
              [4, 5, 6]])
B = np.array([[7, 8],
              [9, 10],
              [11, 12]])

# Perform matrix multiplication
result = np.matmul(A, B)
print("Matrix A:\n", A)
print("Matrix B:\n", B)
print("Result of matrix multiplication (A x B):\n", result)

# Create a vector containing raw logits for a 2-word vocabulary
logits = np.array([2.5, -1.3])
print("Raw logits for 2-word vocabulary:", logits)

# Define a softmax function
def softmax(x):
    exps = np.exp(x - np.max(x))  # for numerical stability
    return exps / np.sum(exps)

# Convert logits into a probability distribution using softmax
probs = softmax(logits)
print("Probability distribution (softmax):", probs)

# Define cross entropy loss function
def cross_entropy_loss(probs, target_index):
    """
    Computes the cross entropy loss given probabilities and the index of the true class.

    probs: numpy array of probabilities (output of softmax)
    target_index: integer, index of the true class
    """
    # Add a small epsilon to prevent log(0)
    epsilon = 1e-15
    p = np.clip(probs[target_index], epsilon, 1. - epsilon)
    return -np.log(p)

# Example: Suppose the true class is index 0
target_index = 0
loss = cross_entropy_loss(probs, target_index)
print("Cross entropy loss for true class at index", target_index, ":", loss)
