#Notes:
# Scalar (0D tensore) / vector (1D) / matrix(2D) / tensore multi d array
# Matrix multiplication / dot product: measure of similarity / attention mechanism
# Goal of LLM / optimization problem / make loss function as low as possible / 
# Derivatives: Measures rate of change of a function / slope of the hill in single direction
# Gradient: Multi D generalization of derivatives / vector that points in the direction of the steepest ascent from current position 
# Training Process: Init weights randomly / Feed data / loss function using true answers / calculate gradient / update weights by subtracting small function of its corresponding gradient / repeat
# Chain rule: Propogate error signal backwards / 
# Softmax: Convert logits to probability distribution / steps: exponentiate every element (making +ve) and then divide each element by the sum of all exponentiated elements / highest logits - highest probability
# Logits: Final output of model will be a vector of raw uncalibrated numbers for every work in vocab 
# Cross Entropy loss: measures distance between predicted n true value / 

import numpy as np

# Create a 1-dimensional NumPy array with values from 0 to 9
arr = np.arange(10)
print("1D array:", arr)

# Create a 2-dimensional NumPy array with shape (2, 5)
arr_2d = np.arange(10).reshape((2, 5))
print("2D array:\n", arr_2d)

# Create two 2D arrays for matrix multiplication (shape: 2x3 and 3x2)
A = np.array([[1, 2, 3],
              [4, 5, 6]])
B = np.array([[7, 8],
              [9, 10],
              [11, 12]])

# Perform matrix multiplication
result = np.matmul(A, B)
print("Matrix A:\n", A)
print("Matrix B:\n", B)
print("Result of matrix multiplication (A x B):\n", result)

# Create a vector containing raw logits for a 2-word vocabulary
logits = np.array([2.5, -1.3])
print("Raw logits for 2-word vocabulary:", logits)

# Define a softmax function
def softmax(x):
    exps = np.exp(x - np.max(x))  # for numerical stability
    return exps / np.sum(exps)

# Convert logits into a probability distribution using softmax
probs = softmax(logits)
print("Probability distribution (softmax):", probs)

# Define cross entropy loss function
def cross_entropy_loss(probs, target_index):
    """
    Computes the cross entropy loss given probabilities and the index of the true class.

    probs: numpy array of probabilities (output of softmax)
    target_index: integer, index of the true class
    """
    # Add a small epsilon to prevent log(0)
    epsilon = 1e-15
    p = np.clip(probs[target_index], epsilon, 1. - epsilon)
    return -np.log(p)

# Example: Suppose the true class is index 0
target_index = 0
loss = cross_entropy_loss(probs, target_index)
print("Cross entropy loss for true class at index", target_index, ":", loss)
